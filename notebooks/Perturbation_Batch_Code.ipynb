{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import site\n",
    "os.sys.path.insert(0, '/home/schirrmr/braindecode/code/')\n",
    "os.sys.path.insert(0, '/home/schirrmr/braindecode/code/braindecode/')\n",
    "os.sys.path.insert(0, '/home/schirrmr/code/auto-diagnosis/')\n",
    "%cd /home/schirrmr/\n",
    "# switch to cpu\n",
    "os.environ['THEANO_FLAGS'] = 'floatX=float32,device=cpu,nvcc.fastmath=True'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png' \n",
    "matplotlib.rcParams['font.size'] = 7\n",
    "\n",
    "import matplotlib.lines as mlines\n",
    "import seaborn\n",
    "seaborn.set_style('darkgrid')\n",
    "matplotlib.rcParams['figure.figsize'] = (12.0, 3.0)\n",
    "import logging\n",
    "import importlib\n",
    "importlib.reload(logging) # see https://stackoverflow.com/a/21475297/1469195\n",
    "log = logging.getLogger()\n",
    "log.setLevel('DEBUG')\n",
    "import sys\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                     level=logging.DEBUG, stream=sys.stdout)\n",
    "from numpy.random import RandomState\n",
    "from braindecode.torch_ext.util import np_to_var\n",
    "from autodiag.perturbation import combine_covs, combine_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rng = RandomState(39483498)\n",
    "a = rng.randn(300,50,12)\n",
    "b = rng.randn(300,10,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from braindecode.util import wrap_reshape_apply_fn, cov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "full_cov = wrap_reshape_apply_fn(cov, a,b, 0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "part_covs = [wrap_reshape_apply_fn(cov, a[i:i+150],b[i:i+150], 0,0) for i in range(0,300,150)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cov_1, cov_2 = part_covs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "part_means = [(np.mean(a[i:i+150], axis=0) ,np.mean(b[i:i+150], axis=0)) for i in range(0,300,150)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(mean_1_a, mean_1_b), (mean_2_a, mean_2_b) = part_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_cov.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.abs((cov_1 + cov_2) /2.0 - full_cov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.abs(combine_covs(cov_1, 150, mean_1_a, mean_1_b, \n",
    "                            cov_2, 150, mean_2_a, mean_2_b) - full_cov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "part_vars = [np.var(a[i:i+150], axis=0, ddof=1) for i in range(0,300,150)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var_1, var_2 = part_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_var = np.var(a, axis=0, ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.abs((var_1 + var_2) / 2.0 - full_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(combine_vars(var_1, 150,mean_1_a, var_2, 150, mean_2_a) - full_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hyperoptim.rerun import rerun_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "folder = 'data/models/pytorch/auto-diag/visualize-dummy/5/'\n",
    "filename = os.path.join(folder, 'exp_file.py')\n",
    "content = open(filename, 'r').read()\n",
    "\n",
    "if not \"save_torch_artifact(ex, exp.model.state_dict(), 'model_params.pkl')\\n    return exp\" in content:\n",
    "    \"save_torch_artifact(ex, exp.model.state_dict(), 'model_params.pkl')\\n\\n\" in content \n",
    "    content = content.replace(\"save_torch_artifact(ex, exp.model.state_dict(), 'model_params.pkl')\\n\",\n",
    "                             \"save_torch_artifact(ex, exp.model.state_dict(), 'model_params.pkl')\\n    return exp\")\n",
    "\n",
    "open(filename, 'w').write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = rerun_exp(folder, update_params=dict(only_return_exp=True), \n",
    "          save_experiment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp = ex.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y  = exp.dataset.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from braindecode.torch_ext.util import var_to_np, np_to_var\n",
    "\n",
    "\n",
    "exp.model.load_state_dict(th.load(os.path.join(folder, 'model_params.pkl')))\n",
    "exp.model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now please verify , then you can do visualization stuffs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set, valid_set, test_set = exp.splitter.split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_batches = list(exp.iterator.get_batches(train_set, shuffle=False))\n",
    "train_X_batches = np.concatenate(list(zip(*train_batches))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "monitor = exp.monitors[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = [var_to_np(exp.model(np_to_var(b[0]).cuda())) for b in train_batches]\n",
    "\n",
    "monitor.monitor_set('train', train_preds, None,None,None, train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_batches = list(exp.iterator.get_batches(valid_set, shuffle=False))\n",
    "valid_preds = [var_to_np(exp.model(np_to_var(b[0]).cuda())) for b in valid_batches]\n",
    "\n",
    "monitor.monitor_set('valid', valid_preds, None,None,None, valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches = list(exp.iterator.get_batches(test_set, shuffle=False))\n",
    "test_preds = [var_to_np(exp.model(np_to_var(b[0]).cuda())) for b in test_batches]\n",
    "\n",
    "monitor.monitor_set('test', test_preds, None,None,None, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check validity by manually decreasing beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "model_without_softmax = nn.Sequential()\n",
    "for name, module in exp.model.named_children():\n",
    "    if name == 'softmax':\n",
    "        break\n",
    "    model_without_softmax.add_module(name, module)\n",
    "\n",
    "pred_fn = lambda x: var_to_np(\n",
    "    th.mean(model_without_softmax(np_to_var(x).cuda()), dim=2)[:, :, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20-30 and 5-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_preds = [var_to_np(model_without_softmax(np_to_var(b[0]).cuda())) for b in train_batches]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pred_arr = np.mean(np.concatenate(train_preds, axis=0), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y_batches = np.concatenate(list(zip(*train_batches))[1])\n",
    "abnorm_mask = train_y_batches == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abnorm_preds = train_pred_arr[abnorm_mask,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "some_abnorm_X = train_X_batches[abnorm_mask][:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_preds = abnorm_preds[:30]\n",
    "plt.plot(orig_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import RandomState\n",
    "from braindecode.visualization.perturbation import _amplitude_phase_to_complex\n",
    "from braindecode.datautil.iterators import get_balanced_batches\n",
    "from braindecode.util import wrap_reshape_apply_fn, corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "examples = some_abnorm_X\n",
    "fft_input = np.fft.rfft(examples, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amps = np.abs(fft_input)\n",
    "phases = np.angle(fft_input)\n",
    "perturbation = np.zeros(amps.shape, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freqs = np.fft.rfftfreq(examples.shape[2],1/100.0)\n",
    "perturbation[:,:,np.searchsorted(freqs, 20):np.searchsorted(freqs,30)+1] = 0\n",
    "perturbation[:,:,np.searchsorted(freqs, 5):np.searchsorted(freqs,15)+1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perturbation = np.maximum(-amps, perturbation)\n",
    "new_amps = amps + perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_complex = _amplitude_phase_to_complex(new_amps, phases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_in = np.fft.irfft(new_complex, axis=2).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_preds = np.mean(var_to_np(model_without_softmax(np_to_var(new_in).cuda())), axis=2)[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_diff = new_preds - orig_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(preds_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "model_without_softmax = nn.Sequential()\n",
    "for name, module in exp.model.named_children():\n",
    "    if name =='softmax':\n",
    "        break\n",
    "    model_without_softmax.add_module(name, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_fn = lambda x: var_to_np(th.mean(model_without_softmax(np_to_var(x).cuda()), dim=2)[:,:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from autodiag.perturbation import compute_amplitude_prediction_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "amp_pred_corrs, orig_accuracy, new_accuracies  = compute_amplitude_prediction_correlations(\n",
    "    pred_fn, train_X_batches[:300], n_iterations=20,\n",
    "                                         batch_size=32,original_y=train_y_batches[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_corr = np.mean(amp_pred_corrs, axis=0)\n",
    "\n",
    "\n",
    "fs = 100.0\n",
    "freqs = np.fft.rfftfreq(train_X_batches.shape[2], d=1.0/fs)\n",
    "\n",
    "\n",
    "POS_APPROX = ('angle',\n",
    "    ('A1', (-5.0, 0.0)),\n",
    "    ('A2', (5.0, 0.0)),\n",
    "    ('Fp1', (-3.5, 3.5)),\n",
    "    ('Fp2', (3.5, 3.5)),\n",
    "    ('F7', (-4.0, 2.0)),\n",
    "    ('F3', (-2.0, 2.0)),\n",
    "    ('Fz', (0.0, 2.0)),\n",
    "    ('F4', (2.0, 2.0)),\n",
    "    ('F8', (4.0, 2.0)),\n",
    "    ('C3', (-2.0, 0.0)),\n",
    "    ('Cz', (0.0, 0.0)),\n",
    "    ('C4', (2.0, 0.0)),\n",
    "    ('T3', (-3,0)),\n",
    "    ('T4', (3,0)),\n",
    "    ('P3', (-2.0, -2.0)),\n",
    "    ('Pz', (0.0, -2.0)),\n",
    "    ('P4', (2.0, -2.0)),\n",
    "    ('T5', (-3.5,-2.)),\n",
    "    ('T6', (3.5,-2.)),          \n",
    "    ('O1', (-4, -3.5)),\n",
    "    ('O2', (4, -3.5)))\n",
    "\n",
    "from braindecode.datasets.sensor_positions import get_channelpos, CHANNEL_10_20_APPROX\n",
    "ch_names = ['A1', 'A2', 'C3', 'C4', 'CZ', 'F3', 'F4', 'F7', 'F8', 'FP1',\n",
    "                    'FP2', 'FZ', 'O1', 'O2',\n",
    "                    'P3', 'P4', 'PZ', 'T3', 'T4', 'T5', 'T6']\n",
    "positions = [get_channelpos(name, POS_APPROX) for name in ch_names]\n",
    "positions = np.array(positions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.visualization.plot import ax_scalp\n",
    "\n",
    "start_freq = 14\n",
    "stop_freq = 30\n",
    "\n",
    "i_start = np.searchsorted(freqs,start_freq)\n",
    "i_stop = np.searchsorted(freqs, stop_freq) + 1\n",
    "\n",
    "freq_corr = np.mean(mean_corr[:,i_start:i_stop], axis=1)\n",
    "max_abs_val = np.max(np.abs(freq_corr))\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "class_names = ['Normal', 'Abnormal']\n",
    "for i_class in range(2):\n",
    "    ax = axes[i_class]\n",
    "    ax_scalp(freq_corr[:,i_class], ch_names, chan_pos_list=POS_APPROX, cmap=cm.coolwarm,\n",
    "            vmin=-max_abs_val, vmax=max_abs_val, ax=ax,annotate=True)\n",
    "    ax.set_title(class_names[i_class])\n",
    "\n",
    "\n",
    "start_freq = 7\n",
    "stop_freq = 14\n",
    "\n",
    "i_start = np.searchsorted(freqs,start_freq)\n",
    "i_stop = np.searchsorted(freqs, stop_freq) + 1\n",
    "\n",
    "freq_corr = np.mean(mean_corr[:,i_start:i_stop], axis=1)\n",
    "from braindecode.visualization.plot import ax_scalp\n",
    "\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "class_names = ['Normal', 'Abnormal']\n",
    "for i_class in range(2):\n",
    "    ax = axes[i_class]\n",
    "    ax_scalp(freq_corr[:,i_class], ch_names, chan_pos_list=POS_APPROX, cmap=cm.coolwarm,\n",
    "            vmin=-max_abs_val, vmax=max_abs_val, ax=ax,annotate=True)\n",
    "    ax.set_title(class_names[i_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from braindecode.util import cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from autodiag.perturbation import compute_amplitude_prediction_correlations_batchwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#from braindecode.visualization.perturbation import compute_amplitude_prediction_correlations\n",
    "amp_pred_corrs, orig_accuracy, new_accuracies  = compute_amplitude_prediction_correlations_batchwise(\n",
    "    pred_fn, train_X_batches[:300], n_iterations=20,\n",
    "                                         batch_size=32,original_y=train_y_batches[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_corr = np.mean(amp_pred_corrs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.visualization.plot import ax_scalp\n",
    "\n",
    "start_freq = 14\n",
    "stop_freq = 30\n",
    "\n",
    "i_start = np.searchsorted(freqs,start_freq)\n",
    "i_stop = np.searchsorted(freqs, stop_freq) + 1\n",
    "\n",
    "freq_corr = np.mean(mean_corr[:,i_start:i_stop], axis=1)\n",
    "max_abs_val = np.max(np.abs(freq_corr))\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "class_names = ['Normal', 'Abnormal']\n",
    "for i_class in range(2):\n",
    "    ax = axes[i_class]\n",
    "    ax_scalp(freq_corr[:,i_class], ch_names, chan_pos_list=POS_APPROX, cmap=cm.coolwarm,\n",
    "            vmin=-max_abs_val, vmax=max_abs_val, ax=ax,annotate=True)\n",
    "    ax.set_title(class_names[i_class])\n",
    "\n",
    "\n",
    "start_freq = 7\n",
    "stop_freq = 14\n",
    "\n",
    "i_start = np.searchsorted(freqs,start_freq)\n",
    "i_stop = np.searchsorted(freqs, stop_freq) + 1\n",
    "\n",
    "freq_corr = np.mean(mean_corr[:,i_start:i_stop], axis=1)\n",
    "from braindecode.visualization.plot import ax_scalp\n",
    "\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "class_names = ['Normal', 'Abnormal']\n",
    "for i_class in range(2):\n",
    "    ax = axes[i_class]\n",
    "    ax_scalp(freq_corr[:,i_class], ch_names, chan_pos_list=POS_APPROX, cmap=cm.coolwarm,\n",
    "            vmin=-max_abs_val, vmax=max_abs_val, ax=ax,annotate=True)\n",
    "    ax.set_title(class_names[i_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(freqs, np.mean(mean_corr[:,:,1], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y_batches = np.concatenate(list(zip(*train_batches))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normal_batches = train_X_batches[train_y_batches == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abnormal_batches = train_X_batches[train_y_batches == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normal_bps = np.abs(np.fft.rfft(normal_batches, axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abnormal_bps = np.abs(np.fft.rfft(abnormal_batches, axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_bps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(freqs, np.log(np.median(abnormal_bps, axis=(0,1,3)) / np.median(normal_bps, axis=(0,1,3)) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(freqs, np.log(np.median(normal_bps, axis=(0,3)) / np.median(abnormal_bps, axis=(0,3))).T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_corr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(freqs, mean_corr[:,:,1].T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(freqs, np.median(normal_bps, axis=(0,1,3)))\n",
    "plt.plot(freqs, np.median(abnormal_bps, axis=(0,1,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### search suitable experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperoptim.results import load_data_frame, remove_columns_with_same_value\n",
    "import pandas as pd\n",
    "df = load_data_frame('data/models/pytorch/auto-diag/10-fold/')\n",
    "df = df[df.finished == 1]\n",
    "df = df.fillna('-')\n",
    "df = df.drop('seed', axis=1)\n",
    "df = remove_columns_with_same_value(df)\n",
    "df.runtime = pd.to_timedelta(np.round(df.runtime), unit='s')\n",
    "df = remove_columns_with_same_value(df)\n",
    "print(len(df))\n",
    "df = df.rename(columns=dict(shrink_the_spikes='shrink',\n",
    "                            channel_standardize='chan_std',\n",
    "                            low_cut_hz='low_hz',\n",
    "                            max_abs_val='max_val',\n",
    "                            high_cut_hz='high_hz',\n",
    "                            moving_demean='m_mean',\n",
    "                            moving_standardize='m_std')).drop(['train_loss', 'valid_loss',\n",
    "                                                       'test_loss', 'valid_misclass',\n",
    "                                                       ], axis=1)\n",
    "df.test_misclass = df.test_misclass * 100\n",
    "\n",
    "#df.sort_values(by='test_sample_misclass')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df = df[(df.model_constraint == 'defaultnorm') & (df.model_name == 'deep')\n",
    "      & (df.max_val == 800) & (df.final_conv_length == 1)\n",
    "               &(df.n_start_chans == '-')\n",
    "               & (df.i_test_fold == 9)]\n",
    "reduced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df = df[(df.model_constraint == 'defaultnorm') & (df.model_name == 'shallow')\n",
    "      & (df.max_val == 800) & (df.final_conv_length == 35)\n",
    "               &(df.n_start_chans == '-')\n",
    "               & (df.i_test_fold == 9)]\n",
    "reduced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from braindecode.visualization.perturbation import gaussian_perturbation\n",
    "def compute_amplitude_prediction_correlations(pred_fn, examples, n_iterations,\n",
    "                                              perturb_fn=gaussian_perturbation,\n",
    "                                              batch_size=30,\n",
    "                                              seed=((2017, 7, 10)),\n",
    "                                              original_y=None,\n",
    "                                             ):\n",
    "    \"\"\"\n",
    "    Perturb input amplitudes and compute correlation between amplitude\n",
    "    perturbations and prediction changes when pushing perturbed input through\n",
    "    the prediction function.\n",
    "\n",
    "    For more details, see [EEGDeepLearning]_.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pred_fn: function\n",
    "        Function accepting an numpy input and returning prediction.\n",
    "    examples: ndarray\n",
    "        Numpy examples, first axis should be example axis.\n",
    "    n_iterations: int\n",
    "        Number of iterations to compute.\n",
    "    perturb_fn: function, optional\n",
    "        Function accepting amplitude array and random generator and returning\n",
    "        perturbation. Default is Gaussian perturbation.\n",
    "    batch_size: int, optional\n",
    "        Batch size for computing predictions.\n",
    "    seed: int, optional\n",
    "        Random generator seed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    amplitude_pred_corrs: ndarray\n",
    "        Correlations between amplitude perturbations and prediction changes\n",
    "        for all sensors and frequency bins.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "\n",
    "    .. [EEGDeepLearning] Schirrmeister, R. T., Springenberg, J. T., Fiederer, L. D. J.,\n",
    "       Glasstetter, M., Eggensperger, K., Tangermann, M., ... & Ball, T. (2017).\n",
    "       Deep learning with convolutional neural networks for EEG decoding and\n",
    "       visualization.\n",
    "       arXiv preprint arXiv:1703.05051.\n",
    "    \"\"\"\n",
    "    inds_per_batch = get_balanced_batches(\n",
    "        n_trials=len(examples), rng=None, shuffle=False, batch_size=batch_size)\n",
    "    log.info(\"Compute original predictions...\")\n",
    "    orig_preds = [pred_fn(examples[example_inds])\n",
    "                  for example_inds in inds_per_batch]\n",
    "    orig_preds_arr = np.concatenate(orig_preds)\n",
    "    if original_y is not None:\n",
    "        orig_pred_labels = np.argmax(orig_preds_arr, axis=1)\n",
    "        orig_accuracy = np.mean(orig_pred_labels == original_y)\n",
    "        log.info(\"Original accuracy: {:.2f}...\".format(orig_accuracy))\n",
    "    rng = RandomState(seed)\n",
    "    fft_input = np.fft.rfft(examples, axis=2)\n",
    "    amps = np.abs(fft_input)\n",
    "    phases = np.angle(fft_input)\n",
    "\n",
    "    amp_pred_corrs = []\n",
    "    new_accuracies = []\n",
    "    for i_iteration in range(n_iterations):\n",
    "        log.info(\"Iteration {:d}...\".format(i_iteration))\n",
    "        log.info(\"Sample perturbation...\")\n",
    "        perturbation = perturb_fn(amps, rng)\n",
    "        log.info(\"Compute new amplitudes...\")\n",
    "        # do not allow perturbation to make amplitudes go below\n",
    "        # zero\n",
    "        perturbation = np.maximum(-amps, perturbation)\n",
    "        new_amps = amps + perturbation\n",
    "        log.info(\"Compute new  complex inputs...\")\n",
    "        new_complex = _amplitude_phase_to_complex(new_amps, phases)\n",
    "        log.info(\"Compute new real inputs...\")\n",
    "        new_in = np.fft.irfft(new_complex, axis=2).astype(np.float32)\n",
    "        log.info(\"Compute new predictions...\")\n",
    "        new_preds = [pred_fn(new_in[example_inds])\n",
    "                     for example_inds in inds_per_batch]\n",
    "\n",
    "        new_preds_arr = np.concatenate(new_preds)\n",
    "        if original_y is not None:\n",
    "            new_pred_labels = np.argmax(new_preds_arr, axis=1)\n",
    "            new_accuracy = np.mean(new_pred_labels == original_y)\n",
    "            log.info(\"New accuracy: {:.2f}...\".format(new_accuracy))\n",
    "            new_accuracies.append(new_accuracy)\n",
    "        diff_preds = new_preds_arr - orig_preds_arr\n",
    "\n",
    "        log.info(\"Compute correlation...\")\n",
    "        amp_pred_corr = wrap_reshape_apply_fn(corr, perturbation[:, :, :, 0],\n",
    "                                              diff_preds,\n",
    "                                              axis_a=(0,), axis_b=(0))\n",
    "        amp_pred_corrs.append(amp_pred_corr)\n",
    "    if original_y is not None:\n",
    "        return amp_pred_corrs, orig_accuracy, new_accuracies\n",
    "    else:\n",
    "        return amp_pred_corrs\n",
    "\n",
    "\n",
    "def _amplitude_phase_to_complex(amplitude, phase):\n",
    "    return amplitude * np.cos(phase) + amplitude * np.sin(phase) * 1j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "folder = 'data/models/pytorch/auto-diag/10-fold/138/'\n",
    "filename = os.path.join(folder, 'exp_file.py')\n",
    "content = open(filename, 'r').read()\n",
    "\n",
    "if not \"save_torch_artifact(ex, exp.model.state_dict(), 'model_params.pkl')\\n    return exp\" in content:\n",
    "    \"save_torch_artifact(ex, exp.model.state_dict(), 'model_params.pkl')\\n\\n\" in content \n",
    "    content = content.replace(\"save_torch_artifact(ex, exp.model.state_dict(), 'model_params.pkl')\\n\",\n",
    "                             \"save_torch_artifact(ex, exp.model.state_dict(), 'model_params.pkl')\\n    return exp\")\n",
    "\n",
    "open(filename, 'w').write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = rerun_exp(folder, update_params=dict(only_return_exp=True), \n",
    "          save_experiment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp = ex.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp.dataset.n_recordings = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y  = exp.dataset.load()\n",
    "\n",
    "\n",
    "train_set, valid_set, test_set = exp.splitter.split(X,y)\n",
    "\n",
    "\n",
    "import torch as th\n",
    "from braindecode.torch_ext.util import var_to_np, np_to_var\n",
    "\n",
    "\n",
    "exp.model.load_state_dict(th.load(os.path.join(folder, 'model_params.pkl')))\n",
    "exp.model.eval();\n",
    "\n",
    "\n",
    "train_batches = list(exp.iterator.get_batches(train_set, shuffle=False))\n",
    "train_X_batches = np.concatenate(list(zip(*train_batches))[0])\n",
    "train_y_batches = np.concatenate(list(zip(*train_batches))[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from torch import nn\n",
    "model_without_softmax = nn.Sequential()\n",
    "for name, module in exp.model.named_children():\n",
    "    if name =='softmax':\n",
    "        break\n",
    "    model_without_softmax.add_module(name, module)\n",
    "\n",
    "pred_fn = lambda x: var_to_np(th.mean(model_without_softmax(np_to_var(x).cuda()), dim=2)[:,:,0,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from autodiag.perturbation import compute_amplitude_prediction_correlations\n",
    "amp_pred_corrs, orig_acc, new_accs = compute_amplitude_prediction_correlations(\n",
    "    pred_fn, train_X_batches,  n_iterations=12, batch_size=32,original_y=train_y_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_corr = np.mean(amp_pred_corrs, axis=0)\n",
    "\n",
    "\n",
    "fs = 250.0\n",
    "freqs = np.fft.rfftfreq(train_X_batches.shape[2], d=1.0/fs)\n",
    "start_freq = 14\n",
    "stop_freq = 30\n",
    "\n",
    "i_start = np.searchsorted(freqs,start_freq)\n",
    "i_stop = np.searchsorted(freqs, stop_freq) + 1\n",
    "\n",
    "freq_corr = np.mean(mean_corr[:,i_start:i_stop], axis=1)\n",
    "\n",
    "POS_APPROX = CHANNEL_10_20_APPROX + (('T3', (-3,0)), ('T4', (3,0)),\n",
    "                       ('T5', (-2.5,-2.5)), ('T6', (2.5,-2.5)))\n",
    "\n",
    "from braindecode.datasets.sensor_positions import get_channelpos, CHANNEL_10_20_APPROX\n",
    "ch_names = ['M1', 'M2', 'C3', 'C4', 'CZ', 'F3', 'F4', 'F7', 'F8', 'FP1',\n",
    "                    'FP2', 'FZ', 'O1', 'O2',\n",
    "                    'P3', 'P4', 'PZ', 'T3', 'T4', 'T5', 'T6']\n",
    "positions = [get_channelpos(name, POS_APPROX) for name in ch_names]\n",
    "positions = np.array(positions)\n",
    "\n",
    "max_abs_val = np.max(np.abs(freq_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.visualization.plot import ax_scalp\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12,8))\n",
    "class_names = ['Normal', 'Abnormal']\n",
    "for i_class in range(2):\n",
    "    ax = axes[i_class]\n",
    "    ax_scalp(freq_corr[:,i_class], ch_names, chan_pos_list=POS_APPROX, cmap=cm.coolwarm,\n",
    "            vmin=-max_abs_val, vmax=max_abs_val, ax=ax,annotate=True)\n",
    "    ax.set_title(class_names[i_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(os.path.join(folder, 'perturbation.npy'),\n",
    "        [amp_pred_corrs,orig_acc, new_accs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a,b,c= np.load(os.path.join(folder, 'perturbation.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(c - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
