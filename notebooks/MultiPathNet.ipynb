{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import site\n",
    "os.sys.path.insert(0, '/home/schirrmr/braindecode/code/')\n",
    "os.sys.path.insert(0, '/home/schirrmr/braindecode/code/braindecode/')\n",
    "os.sys.path.insert(0, '/home/schirrmr/code/auto-diagnosis/')\n",
    "%cd /home/schirrmr/\n",
    "# switch to cpu\n",
    "os.environ['THEANO_FLAGS'] = 'floatX=float32,device=cpu,nvcc.fastmath=True'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg' \n",
    "matplotlib.rcParams['figure.figsize'] = (12.0, 4.0)\n",
    "matplotlib.rcParams['font.size'] = 7\n",
    "\n",
    "import matplotlib.lines as mlines\n",
    "import seaborn\n",
    "seaborn.set_style('darkgrid')\n",
    "import logging\n",
    "import importlib\n",
    "importlib.reload(logging) # see https://stackoverflow.com/a/21475297/1469195\n",
    "log = logging.getLogger()\n",
    "log.setLevel('DEBUG')\n",
    "import sys\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                     level=logging.DEBUG, stream=sys.stdout)\n",
    "\n",
    "from braindecode.torch_ext.util import np_to_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.sys.path.insert(0, '/home/schirrmr/code/auto-diagnosis/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from autodiag.dataset import load_data, DiagnosisSet, get_all_sorted_file_names_and_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_file_names, labels = get_all_sorted_file_names_and_labels('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import RandomState\n",
    "rng = RandomState(384734)\n",
    "in_var = np_to_var(rng.randn(3,5,18,2), dtype=np.float32)\n",
    "percentile = th.kthvalue(th.abs(in_var.view(-1)), int(0.95 * in_var.numel()))\n",
    "cutoff = percentile * 2\n",
    "# store das die ganze zeit, und beim forward pass clampe die werte dann zu +-cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lengths = np.load(\n",
    "            '/home/schirrmr/code/auto-diagnosis/sorted-recording-lengths.npy')\n",
    "mask = lengths < 35 * 60\n",
    "cleaned_file_names = np.array(all_file_names)[mask]\n",
    "cleaned_labels = labels[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sec_to_cut = 60\n",
    "preproc_functions = []\n",
    "duration_recording_mins = 3\n",
    "preproc_functions.append(\n",
    "    lambda data, fs: (data[:, int(sec_to_cut * fs):-int(\n",
    "        sec_to_cut * fs)], fs))\n",
    "preproc_functions.append(\n",
    "    lambda data, fs: (data[:, :int(duration_recording_mins * 60 * fs)], fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def compute_min_max_diffs(x, window_len, cuda):\n",
    "    x_var = np_to_var(x)\n",
    "    if x_var.dim() == 2:\n",
    "        x_var = x_var.unsqueeze(0)\n",
    "    if cuda:\n",
    "        x_var = x_var.cuda()\n",
    "\n",
    "    maxs = F.max_pool1d(x_var,window_len, stride=1)\n",
    "    mins = F.max_pool1d(-x_var,window_len, stride=1)\n",
    "\n",
    "    diffs = maxs + mins\n",
    "    return diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_inds = np.int32(np.arange(10))\n",
    "valid_inds = np.int32(np.arange(10,12))\n",
    "test_inds = np.int32(np.arange(12,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import resampy\n",
    "sec_to_cut = 60\n",
    "preproc_functions = []\n",
    "duration_recording_mins = 3\n",
    "sampling_freq = 100\n",
    "preproc_functions.append(\n",
    "    lambda data, fs: (data[:, int(sec_to_cut * fs):-int(\n",
    "        sec_to_cut * fs)], fs))\n",
    "preproc_functions.append(\n",
    "    lambda data, fs: (data[:, :int(duration_recording_mins * 60 * fs)], fs))\n",
    "preproc_functions.append(lambda data, fs: (resampy.resample(data, fs,\n",
    "                                                            sampling_freq,\n",
    "                                                            axis=1,\n",
    "                                                            filter='kaiser_fast'),\n",
    "                                           sampling_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import resampy\n",
    "def preproc_set_further(the_set, start_fs, preproc_functions):\n",
    "    the_set = deepcopy(the_set)\n",
    "    new_X = []\n",
    "    for x in the_set.X:\n",
    "        fs = start_fs\n",
    "        for preproc_fn in preproc_functions:\n",
    "            x, fs = preproc_fn(x, fs)\n",
    "        new_X.append(x)\n",
    "    the_set.X = new_X\n",
    "    return the_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from braindecode.datautil.signal_target import SignalAndTarget\n",
    "def create_set(inds, sensor_types):\n",
    "    X = []\n",
    "    for i in inds:\n",
    "        log.info(\"Load {:s}\".format(cleaned_file_names[i]))\n",
    "        x = load_data(cleaned_file_names[i], preproc_functions, sensor_types=sensor_types)\n",
    "        X.append(x)\n",
    "    y = cleaned_labels[inds].astype(np.int64)\n",
    "    return SignalAndTarget(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from autodiag.iterator import ModifiedIterator\n",
    "from autodiag.batch_modifier import RemoveMinMaxDiff\n",
    "\n",
    "        \n",
    "from autodiag.clean import set_jumps_to_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_types = ['EEG', 'EKG']\n",
    "\n",
    "train_set = create_set(train_inds, sensor_types)\n",
    "valid_set = create_set(valid_inds, sensor_types)\n",
    "test_set = create_set(test_inds, sensor_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_preproc_fn = []#lambda data, fs: (remove_jumps(data,200,400,True,True), fs)]\n",
    "start_fs = 100\n",
    "#new_preproc_fn.append(lambda data, fs : (np.clip(data,-200,200),fs))\n",
    "new_train_set = preproc_set_further(train_set, start_fs, new_preproc_fn)\n",
    "new_valid_set = preproc_set_further(valid_set, start_fs, new_preproc_fn)\n",
    "new_test_set = preproc_set_further(test_set, start_fs, new_preproc_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "\n",
    "from braindecode.torch_ext.modules import Expression\n",
    "from braindecode.torch_ext.functions import safe_log, square\n",
    "from braindecode.torch_ext.util import np_to_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.torch_ext.util import set_random_seeds\n",
    "from braindecode.models.shallow_fbcsp import ShallowFBCSPNet\n",
    "from autodiag.threepathnet import create_multi_start_path_net\n",
    "cuda = True\n",
    "\n",
    "set_random_seeds(seed=20170629, cuda=cuda)\n",
    "input_time_length = 1200\n",
    "in_chans = 22\n",
    "early_bnorm = False\n",
    "extra_conv_stride = 4\n",
    "later_kernel_len = 5\n",
    "mean_across_features = True\n",
    "n_classifier_filters = 100\n",
    "n_start_filters = 10\n",
    "drop_prob = 0.5\n",
    "virtual_chan_1x1_conv = True\n",
    "\n",
    "\n",
    "model = create_multi_start_path_net(in_chans=in_chans,\n",
    "        virtual_chan_1x1_conv=virtual_chan_1x1_conv, \n",
    "        n_start_filters=n_start_filters, early_bnorm=early_bnorm,\n",
    "        extra_conv_stride=extra_conv_stride, mean_across_features=mean_across_features,\n",
    "        later_kernel_len=later_kernel_len,\n",
    "        n_classifier_filters=n_classifier_filters, drop_prob=drop_prob)\n",
    "\n",
    "\n",
    "\n",
    "test_input = np_to_var(\n",
    "    np.ones((2, in_chans, input_time_length, 1), dtype=np.float32))\n",
    "model(test_input).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "optimizer = optim.Adam(model.parameters())#(model.parameters(), grad_clip_std_factor=2)#optim.Adam(model.parameters())\n",
    "log.info(\"Model:\\n{:s}\".format(str(model)))\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "n_preds_per_input = input_time_length // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "class CosineWithWarmRestarts(object):\n",
    "    def __init__(self, optimizer, batch_period, base_lr, base_wd = 0, update_wd = 0):\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.base_lr = base_lr\n",
    "        self.base_wd = base_wd\n",
    "        self.batch_iteration = 0\n",
    "        self.batch_period = batch_period\n",
    "        self.update_wd = update_wd\n",
    "        self.m_mult = 1.0\n",
    "        self.t_mult = 1.0\n",
    "\n",
    "    def step(self, batch_iteration=None):\n",
    "        if batch_iteration is None:\n",
    "            self.batch_iteration = self.batch_iteration = self.batch_iteration + 1\n",
    "        else:\n",
    "            self.batch_iteration = batch_iteration\n",
    "        tt = self.batch_iteration / self.batch_period\n",
    "        multiplier = 0.5 * (1.0 + math.cos(tt * math.pi))\n",
    "        cur_lr = self.base_lr * multiplier\n",
    "        cur_wd = self.base_wd * multiplier\n",
    "        for i, param_group in enumerate(self.optimizer.param_groups):\n",
    "            param_group['lr'] = cur_lr\n",
    "            if (self.update_wd == 1):    param_group['weight_decay'] = cur_wd\n",
    "        return cur_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ScheduledOptimizer(object):\n",
    "    def __init__(self, scheduler):\n",
    "        assert hasattr(scheduler, 'optimizer')\n",
    "        self.scheduler = scheduler\n",
    "    def state_dict(self):\n",
    "        self.scheduler.optimizer.state_dict()\n",
    "        \n",
    "    def step(self):\n",
    "        self.scheduler.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "    def zero_grad(self):\n",
    "        self.scheduler.optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.datautil.iterators import CropsFromTrialsIterator\n",
    "batch_size = 32\n",
    "log.info(\"{:d} predictions per input/trial\".format(n_preds_per_input))\n",
    "iterator = CropsFromTrialsIterator(batch_size=batch_size,\n",
    "                                   input_time_length=input_time_length,\n",
    "                                  n_preds_per_input=n_preds_per_input)\n",
    "\n",
    "init_lr = 0.1\n",
    "\n",
    "n_batches = sum([1 for _ in iterator.get_batches(train_set, shuffle=False)])\n",
    "max_epochs = 20\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "from autodiag.monitors import CroppedDiagnosisMonitor\n",
    "from braindecode.experiments.experiment import Experiment\n",
    "from braindecode.datautil.iterators import CropsFromTrialsIterator\n",
    "from braindecode.experiments.monitors import (RuntimeMonitor, LossMonitor,\n",
    "                                              CroppedTrialMisclassMonitor,\n",
    "                                              MisclassMonitor)\n",
    "from braindecode.experiments.stopcriteria import MaxEpochs\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), momentum=momentum, lr=init_lr)\n",
    "loss_function = lambda preds, targets: F.nll_loss(th.mean(preds, dim=2, keepdim=False),\n",
    "                                                  targets)\n",
    "model_constraint = None\n",
    "monitors = [LossMonitor(), MisclassMonitor(col_suffix='sample_misclass'),\n",
    "            CroppedDiagnosisMonitor(input_time_length, n_preds_per_input),\n",
    "            RuntimeMonitor(),]\n",
    "stop_criterion = MaxEpochs(20)\n",
    "optimizer = ScheduledOptimizer(CosineWithWarmRestarts(optimizer,batch_period=max_epochs*n_batches,\n",
    "                                                      base_lr=init_lr))\n",
    "batch_modifier=None\n",
    "exp = Experiment(model, new_train_set, new_valid_set, new_test_set, iterator,\n",
    "                 loss_function, optimizer, model_constraint,\n",
    "                 monitors, stop_criterion,\n",
    "                 remember_best_column='valid_misclass',\n",
    "                 run_after_early_stop=False, batch_modifier=batch_modifier, cuda=cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(100 * 100 * 60 * 20 * 2) / (1024 ** 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.setup_training()\n",
    "exp.run_until_first_stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from autodiag.monitors import compute_preds_per_trial\n",
    "from braindecode.torch_ext.util import var_to_np\n",
    "for setname in ('train', 'valid', 'test'):\n",
    "    dataset = exp.datasets[setname]\n",
    "    preds_per_batch = [var_to_np(exp.model(np_to_var(b[0]).cuda()))\n",
    "              for b in exp.iterator.get_batches(dataset, shuffle=False)]\n",
    "    preds_per_trial = compute_preds_per_trial(\n",
    "        preds_per_batch, dataset,\n",
    "        input_time_length=exp.iterator.input_time_length,\n",
    "        n_stride=exp.iterator.n_preds_per_input)\n",
    "    mean_preds_per_trial = [np.mean(preds, axis=(0, 2)) for preds in\n",
    "                                preds_per_trial]\n",
    "    mean_preds_per_trial = np.array(mean_preds_per_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load('data/models/pytorch/auto-diag/dummy-save-pred/7/test_trial_preds.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(exp.epochs_df.train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(exp.epochs_df.train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(exp.epochs_df.train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(exp.epochs_df.train_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
