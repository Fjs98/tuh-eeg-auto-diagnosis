{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import site\n",
    "os.sys.path.insert(0, '/home/schirrmr/braindecode/code/')\n",
    "os.sys.path.insert(0, '/home/schirrmr/braindecode/code/braindecode/')\n",
    "os.sys.path.insert(0, '/home/schirrmr/code/auto-diagnosis/')\n",
    "%cd /home/schirrmr/\n",
    "# switch to cpu\n",
    "os.environ['THEANO_FLAGS'] = 'floatX=float32,device=cpu,nvcc.fastmath=True'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg' \n",
    "matplotlib.rcParams['figure.figsize'] = (12.0, 4.0)\n",
    "matplotlib.rcParams['font.size'] = 7\n",
    "\n",
    "import matplotlib.lines as mlines\n",
    "import seaborn\n",
    "seaborn.set_style('darkgrid')\n",
    "import logging\n",
    "import importlib\n",
    "importlib.reload(logging) # see https://stackoverflow.com/a/21475297/1469195\n",
    "log = logging.getLogger()\n",
    "log.setLevel('DEBUG')\n",
    "import sys\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                     level=logging.DEBUG, stream=sys.stdout)\n",
    "\n",
    "from braindecode.torch_ext.util import np_to_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.sys.path.insert(0, '/home/schirrmr/code/auto-diagnosis/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from autodiag.dataset import load_data, DiagnosisSet, get_all_sorted_file_names_and_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_file_names, labels = get_all_sorted_file_names_and_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import RandomState\n",
    "rng = RandomState(384734)\n",
    "in_var = np_to_var(rng.randn(3,5,18,2), dtype=np.float32)\n",
    "percentile = th.kthvalue(th.abs(in_var.view(-1)), int(0.95 * in_var.numel()))\n",
    "cutoff = percentile * 2\n",
    "# store das die ganze zeit, und beim forward pass clampe die werte dann zu +-cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lengths = np.load(\n",
    "            '/home/schirrmr/code/auto-diagnosis/sorted-recording-lengths.npy')\n",
    "mask = lengths < 35 * 60\n",
    "cleaned_file_names = np.array(all_file_names)[mask]\n",
    "cleaned_labels = labels[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sec_to_cut = 60\n",
    "preproc_functions = []\n",
    "duration_recording_mins = 3\n",
    "preproc_functions.append(\n",
    "    lambda data, fs: (data[:, int(sec_to_cut * fs):-int(\n",
    "        sec_to_cut * fs)], fs))\n",
    "preproc_functions.append(\n",
    "    lambda data, fs: (data[:, :int(duration_recording_mins * 60 * fs)], fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def compute_min_max_diffs(x, window_len, cuda):\n",
    "    x_var = np_to_var(x)\n",
    "    if x_var.dim() == 2:\n",
    "        x_var = x_var.unsqueeze(0)\n",
    "    if cuda:\n",
    "        x_var = x_var.cuda()\n",
    "\n",
    "    maxs = F.max_pool1d(x_var,window_len, stride=1)\n",
    "    mins = F.max_pool1d(-x_var,window_len, stride=1)\n",
    "\n",
    "    diffs = maxs + mins\n",
    "    return diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_inds = np.int32(np.arange(10))\n",
    "valid_inds = np.int32(np.arange(10,12))\n",
    "test_inds = np.int32(np.arange(12,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import resampy\n",
    "sec_to_cut = 60\n",
    "preproc_functions = []\n",
    "duration_recording_mins = 3\n",
    "sampling_freq = 100\n",
    "preproc_functions.append(\n",
    "    lambda data, fs: (data[:, int(sec_to_cut * fs):-int(\n",
    "        sec_to_cut * fs)], fs))\n",
    "preproc_functions.append(\n",
    "    lambda data, fs: (data[:, :int(duration_recording_mins * 60 * fs)], fs))\n",
    "preproc_functions.append(lambda data, fs: (resampy.resample(data, fs,\n",
    "                                                            sampling_freq,\n",
    "                                                            axis=1,\n",
    "                                                            filter='kaiser_fast'),\n",
    "                                           sampling_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import resampy\n",
    "def preproc_set_further(the_set, start_fs, preproc_functions):\n",
    "    the_set = deepcopy(the_set)\n",
    "    new_X = []\n",
    "    for x in the_set.X:\n",
    "        fs = start_fs\n",
    "        for preproc_fn in preproc_functions:\n",
    "            x, fs = preproc_fn(x, fs)\n",
    "        new_X.append(x)\n",
    "    the_set.X = new_X\n",
    "    return the_set\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from braindecode.datautil.signal_target import SignalAndTarget\n",
    "def create_set(inds):\n",
    "    X = []\n",
    "    for i in inds:\n",
    "        log.info(\"Load {:s}\".format(cleaned_file_names[i]))\n",
    "        x = load_data(cleaned_file_names[i], preproc_functions)\n",
    "        X.append(x)\n",
    "    y = cleaned_labels[inds].astype(np.int64)\n",
    "    return SignalAndTarget(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from autodiag.iterator import ModifiedIterator\n",
    "from autodiag.batch_modifier import RemoveMinMaxDiff\n",
    "\n",
    "        \n",
    "from autodiag.clean import set_jumps_to_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = create_set(train_inds)\n",
    "valid_set = create_set(valid_inds)\n",
    "test_set = create_set(test_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_preproc_fn = []#lambda data, fs: (remove_jumps(data,200,400,True,True), fs)]\n",
    "start_fs = 100\n",
    "#new_preproc_fn.append(lambda data, fs : (np.clip(data,-200,200),fs))\n",
    "new_train_set = preproc_set_further(train_set, start_fs, new_preproc_fn)\n",
    "new_valid_set = preproc_set_further(valid_set, start_fs, new_preproc_fn)\n",
    "new_test_set = preproc_set_further(test_set, start_fs, new_preproc_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from braindecode.torch_ext.modules import Expression\n",
    "from torch import nn\n",
    "def net_with_more_layers(net, n_blocks_to_add,\n",
    "                        later_pool_class):\n",
    "    def add_conv_pool_block(model, n_filters_before,\n",
    "                            n_filters, filter_length, block_nr):\n",
    "        suffix = '_{:d}'.format(block_nr)\n",
    "        model.add_module('drop' + suffix,\n",
    "                         nn.Dropout(p=net.drop_prob))\n",
    "        model.add_module('conv' + suffix.format(block_nr),\n",
    "                         nn.Conv2d(n_filters_before, n_filters,\n",
    "                                   (filter_length, 1),\n",
    "                                   stride=1, bias=not net.batch_norm))\n",
    "        if net.batch_norm:\n",
    "            model.add_module('bnorm' + suffix,\n",
    "                         nn.BatchNorm2d(n_filters,\n",
    "                                        momentum=net.batch_norm_alpha,\n",
    "                                        affine=True,\n",
    "                                        eps=1e-5))\n",
    "        model.add_module('nonlin' + suffix,\n",
    "                         Expression(net.later_nonlin))\n",
    "\n",
    "        model.add_module('pool' + suffix,\n",
    "                         later_pool_class(\n",
    "                             kernel_size=(net.pool_time_length, 1),\n",
    "                             stride=(net.pool_time_stride, 1)))\n",
    "        model.add_module('pool_nonlin' + suffix,\n",
    "                         Expression(net.later_pool_nonlin))\n",
    "\n",
    "    model = net.create_network()\n",
    "    from torch import nn\n",
    "    larger_model = nn.Sequential()\n",
    "    for name, module in model.named_children():\n",
    "            if name == 'conv_classifier':\n",
    "                break\n",
    "            larger_model.add_module(name, module)\n",
    "    for i_block in range(n_blocks_to_add):\n",
    "        add_conv_pool_block(larger_model, net.n_filters_4, net.n_filters_4,\n",
    "                            10,i_block+5)\n",
    "\n",
    "    after_classifier = False\n",
    "    for name, module in model.named_children():\n",
    "            if name == 'conv_classifier':\n",
    "                after_classifier = True\n",
    "            if after_classifier:\n",
    "                larger_model.add_module(name, module)\n",
    "    return larger_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from braindecode.torch_ext.util import set_random_seeds\n",
    "from braindecode.models.deep4 import Deep4Net\n",
    "cuda = True\n",
    "input_time_length = 1200\n",
    "final_conv_length = 1\n",
    "set_random_seeds(seed=20170629, cuda=cuda)\n",
    "# This will determine how many crops are processed in parallel\n",
    "n_classes = 2\n",
    "in_chans = 21\n",
    "pool_stride = 3\n",
    "net =  Deep4Net(in_chans=in_chans, n_classes=n_classes,\n",
    "                input_time_length=input_time_length,\n",
    "                final_conv_length=final_conv_length,\n",
    "                pool_time_length=pool_stride,\n",
    "                pool_time_stride=pool_stride,\n",
    "                n_filters_2=50, n_filters_3=80,\n",
    "                n_filters_4=120,\n",
    "                )\n",
    "model = net_with_more_layers(net, 3, nn.MaxPool2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from braindecode.models.util import to_dense_prediction_model\n",
    "input_time_length = 18000\n",
    "optimizer = optim.Adam(model.parameters())#(model.parameters(), grad_clip_std_factor=2)#optim.Adam(model.parameters())\n",
    "to_dense_prediction_model(model)\n",
    "log.info(\"Model:\\n{:s}\".format(str(model)))\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "# determine output size\n",
    "test_input = np_to_var(\n",
    "    np.ones((2, in_chans, input_time_length, 1), dtype=np.float32))\n",
    "if cuda:\n",
    "    test_input = test_input.cuda()\n",
    "out = model(test_input)\n",
    "n_preds_per_input = out.cpu().data.numpy().shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from braindecode.experiments.experiment import Experiment\n",
    "from braindecode.datautil.iterators import CropsFromTrialsIterator\n",
    "from braindecode.experiments.monitors import (RuntimeMonitor, LossMonitor,\n",
    "                                              CroppedTrialMisclassMonitor,\n",
    "                                              MisclassMonitor)\n",
    "from braindecode.experiments.stopcriteria import MaxEpochs\n",
    "\n",
    "batch_size = 32\n",
    "log.info(\"{:d} predictions per input/trial\".format(n_preds_per_input))\n",
    "iterator = CropsFromTrialsIterator(batch_size=batch_size,\n",
    "                                   input_time_length=input_time_length,\n",
    "                                  n_preds_per_input=n_preds_per_input)\n",
    "loss_function = lambda preds, targets: F.nll_loss(\n",
    "            th.mean(preds, dim=2)[:,:,0], targets)\n",
    "model_constraint = None\n",
    "monitors = [LossMonitor(), MisclassMonitor(col_suffix='sample_misclass'),\n",
    "            CroppedTrialMisclassMonitor(input_time_length),\n",
    "            RuntimeMonitor(),]\n",
    "stop_criterion = MaxEpochs(20)\n",
    "batch_modifier = RemoveMinMaxDiff(300,clip_max_abs=True, set_zero=True)#None\n",
    "iterator = ModifiedIterator(iterator, batch_modifier)\n",
    "batch_modifier=None\n",
    "exp = Experiment(model, new_train_set, new_valid_set, new_test_set, iterator,\n",
    "                 loss_function, optimizer, model_constraint,\n",
    "                 monitors, stop_criterion,\n",
    "                 remember_best_column='valid_misclass',\n",
    "                 run_after_early_stop=False, batch_modifier=batch_modifier, cuda=cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.setup_training()\n",
    "exp.run_until_early_stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(np.array(train_set.X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(np.array(new_train_set.X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(exp.epochs_df.train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(exp.epochs_df.train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(exp.epochs_df.train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(exp.epochs_df.train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flat_test_X = np.array(test_set.X).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.distplot(flat_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.histogram(flat_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.histogram(flat_test_X, density=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
